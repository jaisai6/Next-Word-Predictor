{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19EE10050_Assn1_NLP_A21.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3a35tmEySCx7",
        "xhkmGsSoV0zG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z_wN2v1RT1F"
      },
      "source": [
        "# **Assignment-1 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 4th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \n",
        "\n",
        "#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a35tmEySCx7"
      },
      "source": [
        "### **Submission Details:**\n",
        "Name: Jaisaikrishnan\n",
        "\n",
        "Roll No.: 19EE10050\n",
        "\n",
        "Department: EE\n",
        "\n",
        "Email-ID: jaisaikrishnan6@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weHMmyd8fnq"
      },
      "source": [
        "## **Reading a Raw Text Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSy_LOK2aGQ"
      },
      "source": [
        "Retrieve & save raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rku6rV2ORpZA"
      },
      "source": [
        "# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\n",
        "# both inclusive, from the link below:\n",
        "# \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "# Save this corpus in a text file, named as 'rawCorpus.txt'\n",
        "# Print the total number of characters in the text file \n",
        "\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "\n",
        "file = urllib.request.urlopen(url)\n",
        "\n",
        "content_list = file.readlines()\n",
        "\n",
        "\n",
        "raw_text = \"\"\n",
        "\n",
        "start = 0\n",
        "\n",
        "for line in content_list:\n",
        "\n",
        "  if 'CHAPTER XI' in line.decode('utf-8'):\n",
        "    break\n",
        "  \n",
        "  if 'CHAPTER I' in line.decode('utf-8'):\n",
        "    start = 1\n",
        "    continue\n",
        "  \n",
        "  if \"CHAPTER\" in line.decode(\"utf-8\"):\n",
        "    continue\n",
        "\n",
        "  if start == 1:\n",
        "    raw_text  = raw_text + line.decode('utf-8')\n",
        "  \n",
        "# raw_text = raw_text.strip()\n",
        "\n",
        "f = open(\"19EE10050_Assn1_rawCorpus.txt\", \"w\")\n",
        "\n",
        "f.write(raw_text)\n",
        "\n",
        "file.close()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZIOy0Y2hzQ"
      },
      "source": [
        "Read the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsdBJa_l2l7g",
        "outputId": "df10b7a8-b5c5-4c0f-84da-f4b5c4e4bfe6"
      },
      "source": [
        "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
        "# *** Write code ***\n",
        "from pathlib import Path\n",
        "\n",
        "rawReadCorpus = Path('19EE10050_Assn1_rawCorpus.txt').read_text()\n",
        "\n",
        "print (\"Total # of characters in read dataset: {}\".format(len(rawReadCorpus)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # of characters in read dataset: 148542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhkmGsSoV0zG"
      },
      "source": [
        "## **Installing NLTK**\n",
        "\n",
        "The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \n",
        "\n",
        "Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\n",
        "\n",
        "To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\n",
        "```bash\n",
        "conda create -n myenv python=3.6\n",
        "conda activate myenv\n",
        "```\n",
        "\n",
        "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\n",
        "```bash\n",
        "sudo pip3 install nltk \n",
        "python3 \n",
        "nltk.download()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKtZeHq4N98"
      },
      "source": [
        "## **Preprocessing the corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LSUX__82Ff"
      },
      "source": [
        "**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \"\\_will\\_\" (that should ideally appear as \"will\"), \"wouldn't\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7eO4Dm4jIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c52c9e-a367-4736-ce9e-7272376e9888"
      },
      "source": [
        "# Importing modules\n",
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWIzYXyz9Zt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb66969-778a-4d69-f5c2-e1de8e00d3e4"
      },
      "source": [
        "# *** Write code for preprocessing the corpus ***\n",
        "import re\n",
        "\n",
        "# *******************************************************************************************\n",
        "\n",
        "# Removes extra spaces and new lines and other unnecessary noise in the corpus\n",
        "def text_preprocess(corpus):\n",
        "  text = corpus.lower()\n",
        "\n",
        "  newline = re.compile(r\"\\n\")\n",
        "  text = newline.sub(repl = \" \", string = text)\n",
        "\n",
        "  double_newline = re.compile(r\"\\n\\n\")\n",
        "  text = double_newline.sub(repl = \" \", string = text)\n",
        "\n",
        "  white = re.compile(r\"\\s+\")\n",
        "  text = white.sub(repl = \" \", string = text)\n",
        "\n",
        "  text = text.strip(\" \")\n",
        "\n",
        "  url = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n",
        "  text = url.sub(r'', text)\n",
        "    \n",
        "  html = re.compile(r'<.*?>')\n",
        "  text = html.sub(r'', text)\n",
        "\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\U0001F600-\\U0001F64F\"\n",
        "                        u\"\\U0001F300-\\U0001F5FF\"\n",
        "                        u\"\\U0001F680-\\U0001F6FF\"\n",
        "                        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                        u\"\\U00002702-\\U000027B0\"\n",
        "                        u\"\\U000024C2-\\U0001F251\"\n",
        "                        \"]+\", flags = re.UNICODE)\n",
        "  \n",
        "  text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "  word_list = word_tokenize(text)\n",
        "  sentence_list = sent_tokenize(text)\n",
        "\n",
        "  return word_list, sentence_list, text\n",
        "\n",
        "# *******************************************************************************************\n",
        "\n",
        "# Process the word_tokens generated by removing the punctuations present in the tokens.\n",
        "def process_word_list(word_list):\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in word_list]\n",
        "\n",
        "  word_list = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "  return word_list\n",
        "\n",
        "# *******************************************************************************************\n",
        "\n",
        "word_list, sentence_list, pre_processed_corpus = text_preprocess(rawReadCorpus)\n",
        "\n",
        "word_list = process_word_list(word_list)\n",
        "\n",
        "# *******************************************************************************************\n",
        "\n",
        "# Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
        "print(\"First 5 sentences of the pre_processed_corpus: \")\n",
        "for i in range(5):\n",
        "  print(f'{i+1}. {word_list[i]}')\n",
        "\n",
        "print()\n",
        "\n",
        "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
        "print(\"First 5 words of the pre_processed_corpus: \")\n",
        "for i in range(5):\n",
        "  print(f'{i+1}. {sentence_list[i]}')\n",
        "\n",
        "# *******************************************************************************************"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences of the pre_processed_corpus: \n",
            "1. treats\n",
            "2. of\n",
            "3. the\n",
            "4. place\n",
            "5. where\n",
            "\n",
            "First 5 words of the pre_processed_corpus: \n",
            "1. treats of the place where oliver twist was born and of the circumstances attending his birth among other public buildings in a certain town, which for many reasons it will be prudent to refrain from mentioning, and to which i will assign no fictitious name, there is one anciently common to most towns, great or small: to wit, a workhouse; and in this workhouse was born; on a day and date which i need not trouble myself to repeat, inasmuch as it can be of no possible consequence to the reader, in this stage of the business at all events; the item of mortality whose name is prefixed to the head of this chapter.\n",
            "2. for a long time after it was ushered into this world of sorrow and trouble, by the parish surgeon, it remained a matter of considerable doubt whether the child would survive to bear any name at all; in which case it is somewhat more than probable that these memoirs would never have appeared; or, if they had, that being comprised within a couple of pages, they would have possessed the inestimable merit of being the most concise and faithful specimen of biography, extant in the literature of any age or country.\n",
            "3. although i am not disposed to maintain that the being born in a workhouse, is in itself the most fortunate and enviable circumstance that can possibly befall a human being, i do mean to say that in this particular instance, it was the best thing for oliver twist that could by possibility have occurred.\n",
            "4. the fact is, that there was considerable difficulty in inducing oliver to take upon himself the office of respiration,â€”a troublesome practice, but one which custom has rendered necessary to our easy existence; and for some time he lay gasping on a little flock mattress, rather unequally poised between this world and the next: the balance being decidedly in favour of the latter.\n",
            "5. now, if, during this brief period, oliver had been surrounded by careful grandmothers, anxious aunts, experienced nurses, and doctors of profound wisdom, he would most inevitably and indubitably have been killed in no time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ75_a1QL70J"
      },
      "source": [
        "**Perform the following tasks for the given corpus:**\n",
        "1. Print the average number of tokens per sentence.\n",
        "2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
        "3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyG0g3oSADmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fae367-9407-4850-94eb-45fd7821f2c8"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydHIxC7lG7Py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b7f3f4-acae-49e2-e2c8-f44da68eb76b"
      },
      "source": [
        "# *** Write code for the 2 tasks above ***\n",
        "\n",
        "# Task - 1\n",
        "total_words = len(word_list)\n",
        "total_sentences = len(sentence_list)\n",
        "\n",
        "tokens_per_sentence = total_words//total_sentences\n",
        "\n",
        "print(f'Average number of tokens per sentence: {tokens_per_sentence}')\n",
        "\n",
        "# Task - 2\n",
        "longest_sentence_len, shortest_sentence_len = float('-inf'), float('inf')\n",
        "\n",
        "longest_sentence, shortest_sentence = \"\", \"\"\n",
        "\n",
        "for sent in sentence_list:\n",
        "  if 'oliver' in sent:\n",
        "    cur_word_list = word_tokenize(sent)\n",
        "    cur_word_list = process_word_list(cur_word_list)\n",
        "\n",
        "    cur_sent_len = len(cur_word_list)\n",
        "\n",
        "    if longest_sentence_len < cur_sent_len:\n",
        "      longest_sentence_len = cur_sent_len\n",
        "      longest_sentence = sent\n",
        "\n",
        "    if shortest_sentence_len > cur_sent_len:\n",
        "      shortest_sentence_len = cur_sent_len\n",
        "      shortest_sentence = sent\n",
        "\n",
        "print(f'The length of the longest sentence that contains \"Oliver\" is : {longest_sentence_len}')\n",
        "print(f'The length of the shortest sentence that contains \"Oliver\" is : {shortest_sentence_len}')\n",
        "\n",
        "# Task - 3\n",
        "stop_words = stopwords.words('english')\n",
        "filtered_words = [word for word in word_list if not word in stop_words]\n",
        "print(f'The number of unique tokens after removal of stopwords is: {len(set(filtered_words))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of tokens per sentence: 24\n",
            "The length of the longest sentence that contains \"Oliver\" is : 114\n",
            "The length of the shortest sentence that contains \"Oliver\" is : 2\n",
            "The number of unique tokens after removal of stopwords is: 4044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RiDR7TJjKX"
      },
      "source": [
        "## **Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJeTSt8HM95L"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the given corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "\n",
        "2. **List the top 10 bigrams, trigrams**\n",
        "(Additionally remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPXGvVaR-ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648099bb-ceb1-4921-9e12-65d9f16d6c68"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "\n",
        "# for content in (#pre_processed corpus): # *** Write code ***\n",
        "#     unigrams.extend(words)\n",
        "#     bigrams.extend(ngrams(content,2))\n",
        "#     ##similar for trigrams \n",
        "#     # *** Write code ***\n",
        "    \n",
        "# Instead of going line by line and extracting the word_tokens, already processed word_list\n",
        "# has been used to find the unigrams, bigrams and trigrams in the corpus.\n",
        "\n",
        "# This is due to the stop iteration error, as sometimes while reading line by line, it may\n",
        "# so happen that, no. of tokens after processing is less than 3. In that case, trigrams\n",
        "# couldn't be created. Thus, to avoid such cases, word_list has been used directly.\n",
        "unigrams.extend(word_list)\n",
        "bigrams.extend(ngrams(word_list,2))\n",
        "trigrams.extend(ngrams(word_list,3))\n",
        "\n",
        "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams[:5]) + \" ...\\n\")\n",
        "\n",
        "# list of unigram, bigram & trigram after removing those that \n",
        "# totally contain only articles, prepositions, determiners\n",
        "# Eg. For bigrams, don't remove items like (\"a\", \"boy\") --> where not all are \n",
        "#     articles, prepositions, determiners\n",
        "#     But remove items like (\"in\", \"the\") --> where all are articles, prepositions, determiners\n",
        "# Similarly, for unigrams and trigrams\n",
        "\n",
        "unigrams_Processed = [word for word in unigrams if not word in stop_words]\n",
        "bigrams_Processed = [word for word in bigrams if not word[0] in stop_words or not word[1] in stop_words]\n",
        "trigrams_Processed = [word for word in trigrams if not word[0] in stop_words or not word[1] in stop_words or not word[2] in stop_words]\n",
        "\n",
        "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
        "\n",
        "def get_ngrams_freqDist(n, ngramList):\n",
        "    #This function computes the frequency corresponding to each ngram in ngramList \n",
        "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
        "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
        "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
        "    \n",
        "    # *** Write code ***\n",
        "    ngram_freq_dict = {ngram:ngramList.count(ngram) for ngram in set(ngramList)}\n",
        "    return ngram_freq_dict\n",
        "\n",
        "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
        "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
        "bigrams_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "trigrams_freqDist = get_ngrams_freqDist(3, trigrams_Processed)\n",
        "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)                                                 \n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"Top 10 unigrams: \")\n",
        "for i in range(10):\n",
        "  print(f'{i+1}. {sorted(unigrams_freqDist.items(), key = lambda y : y[1], reverse=True)[i][0]}')\n",
        "print()\n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"Top 10 unigrams_Processed: \")\n",
        "for i in range(10):\n",
        "  print(f'{i+1}. {sorted(unigrams_Processed_freqDist.items(), key = lambda y : y[1], reverse = True)[i][0]}')\n",
        "print()\n",
        "\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"Top 10 bigrams: \")\n",
        "for i in range(10):\n",
        "  print(f'{i+1}. {sorted(bigrams_freqDist.items(), key = lambda y : y[1], reverse = True)[i][0]}')\n",
        "print()\n",
        "\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"Top 10 bigrams_Processed: \")\n",
        "for i in range(10):\n",
        "  print(f'{i+1}. {sorted(bigrams_Processed_freqDist.items(), key = lambda y : y[1], reverse = True)[i][0]}')\n",
        "print()\n",
        "\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"Top 10 trigrams: \")\n",
        "for i in range(10):\n",
        "  print(f'{i+1}. {sorted(trigrams_freqDist.items(), key = lambda y : y[1], reverse = True)[i][0]}')\n",
        "print()\n",
        "\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"Top 10 trigrams_Processed: \")\n",
        "for i in range(10):\n",
        "  print(f'{i+1}. {sorted(trigrams_Processed_freqDist.items(), key = lambda y : y[1], reverse = True)[i][0]}')\n",
        "print()\n",
        "print(\"---------------------------------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of n-grams:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "['treats', 'of', 'the', 'place', 'where'] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('treats', 'of'), ('of', 'the'), ('the', 'place'), ('place', 'where'), ('where', 'oliver')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "Sample of n-grams after processing:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "['treats', 'place', 'oliver', 'twist', 'born'] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('treats', 'of'), ('the', 'place'), ('place', 'where'), ('where', 'oliver'), ('oliver', 'twist')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "Top 10 unigrams: \n",
            "1. the\n",
            "2. and\n",
            "3. a\n",
            "4. of\n",
            "5. to\n",
            "6. he\n",
            "7. his\n",
            "8. in\n",
            "9. was\n",
            "10. oliver\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "Top 10 unigrams_Processed: \n",
            "1. oliver\n",
            "2. said\n",
            "3. mr\n",
            "4. bumble\n",
            "5. gentleman\n",
            "6. old\n",
            "7. sowerberry\n",
            "8. boy\n",
            "9. would\n",
            "10. replied\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "Top 10 bigrams: \n",
            "1. ('mr', 'bumble')\n",
            "2. ('said', 'the')\n",
            "3. ('the', 'old')\n",
            "4. ('the', 'undertaker')\n",
            "5. ('old', 'gentleman')\n",
            "6. ('the', 'boy')\n",
            "7. ('the', 'jew')\n",
            "8. ('said', 'mr')\n",
            "9. ('the', 'gentleman')\n",
            "10. ('mrs', 'sowerberry')\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "Top 10 bigrams_Processed: \n",
            "1. ('mr', 'bumble')\n",
            "2. ('said', 'the')\n",
            "3. ('the', 'old')\n",
            "4. ('the', 'undertaker')\n",
            "5. ('old', 'gentleman')\n",
            "6. ('the', 'boy')\n",
            "7. ('the', 'jew')\n",
            "8. ('said', 'mr')\n",
            "9. ('the', 'gentleman')\n",
            "10. ('mrs', 'sowerberry')\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "Top 10 trigrams: \n",
            "1. ('the', 'old', 'gentleman')\n",
            "2. ('gentleman', 'in', 'the')\n",
            "3. ('the', 'gentleman', 'in')\n",
            "4. ('the', 'white', 'waistcoat')\n",
            "5. ('said', 'mr', 'bumble')\n",
            "6. ('in', 'the', 'white')\n",
            "7. ('said', 'the', 'undertaker')\n",
            "8. ('said', 'the', 'gentleman')\n",
            "9. ('said', 'the', 'jew')\n",
            "10. ('sir', 'replied', 'oliver')\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "Top 10 trigrams_Processed: \n",
            "1. ('the', 'old', 'gentleman')\n",
            "2. ('gentleman', 'in', 'the')\n",
            "3. ('the', 'gentleman', 'in')\n",
            "4. ('the', 'white', 'waistcoat')\n",
            "5. ('said', 'mr', 'bumble')\n",
            "6. ('in', 'the', 'white')\n",
            "7. ('said', 'the', 'undertaker')\n",
            "8. ('said', 'the', 'gentleman')\n",
            "9. ('said', 'the', 'jew')\n",
            "10. ('sir', 'replied', 'oliver')\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqu8nVV7NREo"
      },
      "source": [
        "## **Next three words' Prediction using Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vnIM26b2WA"
      },
      "source": [
        "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
        "That is, pretend we saw each word one more time than we did.\n",
        "\n",
        "You have two tasks here.\n",
        "\n",
        "First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
        "\n",
        "As for example, for the string 'Raj has a' the answers can be as below: \n",
        "\n",
        "(1) Raj has a **beautiful red car**\n",
        "\n",
        "(2) Raj has a **charismatic magnetic personality**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAGB1_S8NThy"
      },
      "source": [
        "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
        "testSent2 = \"They made room for the stranger, but he sat down\"\n",
        "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLY1ymH-ZuJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66078a0-ad43-4d87-96ca-4a979da0f2ff"
      },
      "source": [
        "# *** Write code ***\n",
        "\n",
        "# ***********************************************************************************************************************\n",
        "\n",
        "def smoothed_bigram_model(unigrams_freqDist, bigrams_freqDist, sentence, no_of_words):\n",
        "\n",
        "    V = len(unigrams_freqDist.keys())\n",
        "\n",
        "    prev_word = sentence.split(' ')[-1]\n",
        "    prev_word = process_word_list([prev_word])[0]\n",
        "\n",
        "    word = \"\"\n",
        "\n",
        "    bigram_predicted_words = []\n",
        "\n",
        "    # Get the required no_of_words by looping no_of_words times.\n",
        "    for i in range(no_of_words):\n",
        "        \n",
        "        prob = 0\n",
        "\n",
        "        for key, value in unigrams_freqDist.items():\n",
        "\n",
        "            bigram = tuple((prev_word, key))\n",
        "            bigram_count = 0\n",
        "\n",
        "            if bigram in bigrams_freqDist.keys():\n",
        "                bigram_count = bigrams_freqDist[bigram]\n",
        "            \n",
        "            unigram_count = 0\n",
        "            if prev_word in unigrams_freqDist.keys():\n",
        "                unigram_count = unigrams_freqDist[prev_word]\n",
        "            \n",
        "            # Calculate the smoothed probability\n",
        "            temp_prob = (bigram_count + 1) / (unigram_count + V)\n",
        "\n",
        "            if temp_prob >= prob:\n",
        "                prob = temp_prob\n",
        "                word = key\n",
        "\n",
        "        # Append the best word found\n",
        "        bigram_predicted_words.append(word)\n",
        "        prev_word = word\n",
        "\n",
        "    return bigram_predicted_words\n",
        "\n",
        "# ***********************************************************************************************************************\n",
        "\n",
        "def smoothed_trigram_model(unigrams_freqDist, bigrams_freqDist, trigrams_freqDist, sentence, no_of_words):\n",
        "\n",
        "    V = len(unigrams_freqDist.keys())\n",
        "\n",
        "    prev_word = sentence.split(' ')[-1]\n",
        "    prev_word = process_word_list([prev_word])[0]\n",
        "\n",
        "    prev_to_prev_word = sentence.split(' ')[-2]\n",
        "    prev_to_prev_word = process_word_list([prev_to_prev_word])[0]\n",
        "\n",
        "    word = \"\"\n",
        "    \n",
        "    trigram_predicted_words = []\n",
        "\n",
        "    # Iterate no_of_words times \n",
        "    for i in range(no_of_words):\n",
        "\n",
        "        prob = 0\n",
        "\n",
        "        for key, value in unigrams_freqDist.items():\n",
        "\n",
        "            trigram = tuple((prev_to_prev_word, prev_word, key))\n",
        "            bigram = tuple((prev_to_prev_word, prev_word))\n",
        "\n",
        "            trigram_count = 0\n",
        "            if trigram in trigrams_freqDist.keys():\n",
        "                trigram_count = trigrams_freqDist[trigram]\n",
        "            \n",
        "            bigram_count = 0\n",
        "            if bigram in bigrams_freqDist.keys():\n",
        "                bigram_count = bigrams_freqDist[bigram]\n",
        "\n",
        "            # Compute the smoothed probability\n",
        "            temp_prob = (trigram_count + 1) / (bigram_count + V)\n",
        "            \n",
        "            if temp_prob >= prob:\n",
        "                prob = temp_prob\n",
        "                word = key\n",
        "\n",
        "        # Append the best word found\n",
        "        trigram_predicted_words.append(word)\n",
        "        prev_to_prev_word = prev_word\n",
        "        prev_word = word\n",
        "\n",
        "    return trigram_predicted_words\n",
        "\n",
        "# ***********************************************************************************************************************\n",
        "# Test Sentence - 1\n",
        "bigram_predicted_words_1 = smoothed_bigram_model(unigrams_freqDist, bigrams_freqDist, testSent1, 3)\n",
        "\n",
        "bigram_predicted_sentence_1 = testSent1 + \" \"\n",
        "for word in bigram_predicted_words_1:\n",
        "  bigram_predicted_sentence_1 = bigram_predicted_sentence_1 + word + \" \"\n",
        "\n",
        "trigram_predicted_words_1 = smoothed_trigram_model(unigrams_freqDist, bigrams_freqDist, trigrams_freqDist, testSent1, 3)\n",
        "\n",
        "trigram_predicted_sentence_1 = testSent1 + \" \"\n",
        "for word in trigram_predicted_words_1:\n",
        "  trigram_predicted_sentence_1 = trigram_predicted_sentence_1 + word + \" \"\n",
        "\n",
        "print(\"Test sent - 1\")\n",
        "print()\n",
        "print(\"Bigram_prediction: \")\n",
        "print(bigram_predicted_sentence_1)\n",
        "print()\n",
        "print(\"Trigram_prediction: \")\n",
        "print(trigram_predicted_sentence_1)\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print()\n",
        "\n",
        "# ***********************************************************************************************************************\n",
        "# Test Sentence - 2\n",
        "bigram_predicted_words_2 = smoothed_bigram_model(unigrams_freqDist, bigrams_freqDist, testSent2, 3)\n",
        "\n",
        "bigram_predicted_sentence_2 = testSent2 + \" \"\n",
        "for word in bigram_predicted_words_2:\n",
        "  bigram_predicted_sentence_2 = bigram_predicted_sentence_2 + word + \" \"\n",
        "\n",
        "trigram_predicted_words_2 = smoothed_trigram_model(unigrams_freqDist, bigrams_freqDist, trigrams_freqDist, testSent2, 3)\n",
        "\n",
        "trigram_predicted_sentence_2 = testSent2 + \" \"\n",
        "for word in trigram_predicted_words_2:\n",
        "  trigram_predicted_sentence_2 = trigram_predicted_sentence_2 + word + \" \"\n",
        "\n",
        "print(\"Test sent - 2\")\n",
        "print()\n",
        "print(\"Bigram_prediction: \")\n",
        "print(bigram_predicted_sentence_2)\n",
        "print()\n",
        "print(\"Trigram_prediction: \")\n",
        "print(trigram_predicted_sentence_2)\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print()\n",
        "\n",
        "# ***********************************************************************************************************************\n",
        "# Test Sentence - 3\n",
        "bigram_predicted_words_3 = smoothed_bigram_model(unigrams_freqDist, bigrams_freqDist, testSent3, 3)\n",
        "\n",
        "bigram_predicted_sentence_3 = testSent3 + \" \"\n",
        "for word in bigram_predicted_words_3:\n",
        "  bigram_predicted_sentence_3 = bigram_predicted_sentence_3 + word + \" \"\n",
        "\n",
        "trigram_predicted_words_3 = smoothed_trigram_model(unigrams_freqDist, bigrams_freqDist, trigrams_freqDist, testSent3, 3)\n",
        "\n",
        "trigram_predicted_sentence_3 = testSent3 + \" \"\n",
        "for word in trigram_predicted_words_3:\n",
        "  trigram_predicted_sentence_3 = trigram_predicted_sentence_3 + word + \" \"\n",
        "\n",
        "print(\"Test sent - 3\")\n",
        "print()\n",
        "print(\"Bigram_prediction: \")\n",
        "print(bigram_predicted_sentence_3)\n",
        "print()\n",
        "print(\"Trigram_prediction: \")\n",
        "print(trigram_predicted_sentence_3)\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print()\n",
        "\n",
        "# ***********************************************************************************************************************"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sent - 1\n",
            "\n",
            "Bigram_prediction: \n",
            "There was a sudden jerk, a terrific convulsion of the limbs; and there he would have known \n",
            "\n",
            "Trigram_prediction: \n",
            "There was a sudden jerk, a terrific convulsion of the limbs; and there he stood for a \n",
            "---------------------------------------------------------------------------------------------\n",
            "\n",
            "Test sent - 2\n",
            "\n",
            "Bigram_prediction: \n",
            "They made room for the stranger, but he sat down upon the old \n",
            "\n",
            "Trigram_prediction: \n",
            "They made room for the stranger, but he sat down to earth again \n",
            "---------------------------------------------------------------------------------------------\n",
            "\n",
            "Test sent - 3\n",
            "\n",
            "Bigram_prediction: \n",
            "The hungry and destitute situation of the infant orphan was duly reported by mr bumble s \n",
            "\n",
            "Trigram_prediction: \n",
            "The hungry and destitute situation of the infant orphan was duly reported by the side of \n",
            "---------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfeaacTdO6h"
      },
      "source": [
        "Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\n",
        "\n",
        "Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMkW9hKecxK"
      },
      "source": [
        "**Answer**: \n",
        "\n",
        "The predicted sentences were not present in the original corpus for all the three test cases. This is due to the fact that, the context of the sentence was not considered during the prediction, instead a simple probabilistic approach was used to find the subsequent words.\n",
        "\n",
        "The speciality of test sentence 3 was that it is not present in our original corpus.\n",
        "The bigram model predicted \"mr.bumble\", which is a proper noun and the word (mr.bumble) is specific to our corpus.  \n",
        "\n",
        "This is due to the fact that, the bigram model only looks at the previous word and try to predict the next word, which sometimes may cause the bigram model to predict in a more specific way, whereas the trigram model looks at previous two words, which results in a more generic prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVBMcaAJXR9S"
      },
      "source": [
        "Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIL0kRyh3p0p",
        "outputId": "4efe5860-5969-4f8f-dcb4-01136b6aca77"
      },
      "source": [
        "def perplexity(model,sentence):\n",
        "\n",
        "  V = len(unigrams_freqDist)\n",
        "\n",
        "  prob = 1\n",
        "\n",
        "  words = word_tokenize(sentence)\n",
        "  words = process_word_list(words)\n",
        "\n",
        "  if model == 'bigram':\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "      cur_word = words[i]\n",
        "      next_word = words[i+1]\n",
        "\n",
        "      bigram = tuple((cur_word, next_word))\n",
        "\n",
        "      bigram_count = 0\n",
        "\n",
        "      if bigram in bigrams_freqDist.keys():\n",
        "          bigram_count = bigrams_freqDist[bigram]\n",
        "      \n",
        "      unigram_count = 0\n",
        "\n",
        "      if cur_word in unigrams_freqDist.keys():\n",
        "          unigram_count = unigrams_freqDist[cur_word]\n",
        "\n",
        "      # Compute the smoothed probability of the sentence\n",
        "      prob = prob * ((bigram_count + 1) / (unigram_count + V))\n",
        "\n",
        "  elif model == 'trigram':\n",
        "\n",
        "    for i in range(len(words) - 2):\n",
        "      cur_word = words[i]\n",
        "      next_word = words[i+1]\n",
        "      next_to_next_word = words[i+2]\n",
        "\n",
        "      bigram = tuple((cur_word, next_word))\n",
        "      trigram = tuple((cur_word, next_word, next_to_next_word))\n",
        "\n",
        "      bigram_count = 0\n",
        "\n",
        "      if bigram in bigrams_freqDist.keys():\n",
        "          bigram_count = bigrams_freqDist[bigram]\n",
        "      \n",
        "      trigram_count = 0\n",
        "\n",
        "      if trigram in trigrams_freqDist.keys():\n",
        "        trigram_count = trigrams_freqDist[trigram]\n",
        "\n",
        "      prob = prob * ((trigram_count + 1) / (bigram_count + V))\n",
        "         \n",
        "  # Calculate the perplexity from the probability\n",
        "  perplexity = (1/prob) ** (1/len(words))\n",
        "\n",
        "  return perplexity\n",
        "\n",
        "# Test sent 1\n",
        "print(\"Test sent - 1\")\n",
        "print()\n",
        "print(\"Bigram_prediction: \")\n",
        "print(bigram_predicted_sentence_1)\n",
        "print()\n",
        "print(\"Perplexity of bigram model: \", perplexity(\"bigram\", bigram_predicted_sentence_1))\n",
        "print()\n",
        "print(\"Trigram_prediction: \")\n",
        "print(trigram_predicted_sentence_1)\n",
        "print()\n",
        "print(\"Perplexity of trigram model: \", perplexity(\"trigram\", trigram_predicted_sentence_1))\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "print()\n",
        "\n",
        "# Test sent 2\n",
        "print(\"Test sent - 2\")\n",
        "print()\n",
        "print(\"Bigram_prediction: \")\n",
        "print(bigram_predicted_sentence_2)\n",
        "print()\n",
        "print(\"Perplexity of bigram model: \", perplexity(\"bigram\", bigram_predicted_sentence_2))\n",
        "print()\n",
        "print(\"Trigram_prediction: \")\n",
        "print(trigram_predicted_sentence_2)\n",
        "print()\n",
        "print(\"Perplexity of trigram model: \", perplexity(\"trigram\", trigram_predicted_sentence_2))\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "print()\n",
        "\n",
        "# Test sent 3\n",
        "print(\"Test sent - 3\")\n",
        "print()\n",
        "print(\"Bigram_prediction: \")\n",
        "print(bigram_predicted_sentence_3)\n",
        "print()\n",
        "print(\"Perplexity of bigram model: \", perplexity(\"bigram\", bigram_predicted_sentence_3))\n",
        "print()\n",
        "print(\"Trigram_prediction: \")\n",
        "print(trigram_predicted_sentence_3)\n",
        "print()\n",
        "print(\"Perplexity of trigram model: \", perplexity(\"trigram\", trigram_predicted_sentence_3))\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sent - 1\n",
            "\n",
            "Bigram_prediction: \n",
            "There was a sudden jerk, a terrific convulsion of the limbs; and there he would have known \n",
            "\n",
            "Perplexity of bigram model:  1669.8818666825284\n",
            "\n",
            "Trigram_prediction: \n",
            "There was a sudden jerk, a terrific convulsion of the limbs; and there he stood for a \n",
            "\n",
            "Perplexity of trigram model:  1388.4991445714768\n",
            "-----------------------------------------------------------------------------------\n",
            "\n",
            "Test sent - 2\n",
            "\n",
            "Bigram_prediction: \n",
            "They made room for the stranger, but he sat down upon the old \n",
            "\n",
            "Perplexity of bigram model:  951.5083819266038\n",
            "\n",
            "Trigram_prediction: \n",
            "They made room for the stranger, but he sat down to earth again \n",
            "\n",
            "Perplexity of trigram model:  880.8130407859459\n",
            "-----------------------------------------------------------------------------------\n",
            "\n",
            "Test sent - 3\n",
            "\n",
            "Bigram_prediction: \n",
            "The hungry and destitute situation of the infant orphan was duly reported by mr bumble s \n",
            "\n",
            "Perplexity of bigram model:  905.0402757776466\n",
            "\n",
            "Trigram_prediction: \n",
            "The hungry and destitute situation of the infant orphan was duly reported by the side of \n",
            "\n",
            "Perplexity of trigram model:  749.5098550993202\n",
            "-----------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAPa1OVZX8uN"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "As we can clearly see that for all the three sentences, the perplexity of the trigram model is lower than the corrosponding bigram model, meaning the probability is higher for trigram model. \n",
        "\n",
        "This also justifies the fact that, trigram model performs better than bigram model in general and thus, trigram model have lower perplexity than the bigram model."
      ]
    }
  ]
}